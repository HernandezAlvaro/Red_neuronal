{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvaro_Hernandez\\AppData\\Local\\Temp\\ipykernel_13892\\3472650159.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import hashlib\n",
    "import torch\n",
    "from Casos.caso import caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BORRAR ARCHIVOS YA EXISTENTES\n",
    "carpetas = ['Data', 'Data_csv', 'Data_pt']\n",
    "\n",
    "for carpeta in carpetas:\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        ruta_archivo = os.path.join(carpeta, archivo)\n",
    "        if os.path.isfile(ruta_archivo):\n",
    "            os.remove(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_casos = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAR ARCHIVOS .fem\n",
    "\n",
    "fem1 = caso(num_casos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron archivos duplicados.\n"
     ]
    }
   ],
   "source": [
    "# ARCHIVOS DUPLICADOS \n",
    "\n",
    "def calcular_hash(nombre_archivo):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(nombre_archivo, 'rb') as archivo:\n",
    "        buffer = archivo.read()\n",
    "        hasher.update(buffer)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def encontrar_duplicados(ruta_directorio):\n",
    "    hash_dict = {}\n",
    "    duplicados = []\n",
    "\n",
    "    for directorio_raiz, _, archivos in os.walk(ruta_directorio):\n",
    "        for nombre_archivo in archivos:\n",
    "            ruta_completa = os.path.join(directorio_raiz, nombre_archivo)\n",
    "            hash_archivo = calcular_hash(ruta_completa)\n",
    "\n",
    "            if hash_archivo in hash_dict:\n",
    "                duplicados.append((ruta_completa, hash_dict[hash_archivo]))\n",
    "            else:\n",
    "                hash_dict[hash_archivo] = ruta_completa\n",
    "\n",
    "    return duplicados\n",
    "\n",
    "# Cambia esta ruta al directorio donde se encuentran tus archivos FEM\n",
    "ruta_directorio = 'Data'\n",
    "duplicados = encontrar_duplicados(ruta_directorio)\n",
    "\n",
    "if duplicados:\n",
    "    print(\"Se encontraron archivos duplicados:\")\n",
    "    for dup in duplicados:\n",
    "        print(\"Archivo 1:\", dup[0])\n",
    "        print(\"Archivo 2:\", dup[1])\n",
    "else:\n",
    "    print(\"No se encontraron archivos duplicados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUTAR OPTISTRUCT \n",
    "for i in range(1,num_casos+1): # Cambiar segun el numero de casos\n",
    "# Ruta al ejecutable de OptiStruct\n",
    "    optistruct_ejecutable = \"D:\\\\Program Files\\\\Altair\\\\2023.1\\\\hwsolvers\\\\scripts\\\\optistruct.bat\"\n",
    "    archivo_ejecutable = f'Data\\\\Data_{i}.fem'\n",
    "# Comando para ejecutar OptiStruct con el archivo FEM\n",
    "    command = [optistruct_ejecutable, archivo_ejecutable]\n",
    "# Ejecutar el comand\n",
    "    try:\n",
    "    # Utilizamos subprocess.run para ejecutar el comando y esperar a que termine\n",
    "        subprocess.run(command, check=True)  \n",
    "    except subprocess.CalledProcessError as e:\n",
    "    # Capturar errores si ocurren\n",
    "        print(\"Error al ejecutar OptiStruct:\", e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BORRAR ARCHIVOS INNECESARIOS\n",
    "for i in range(1,num_casos+1): # Cambiar segun el numero de casos\n",
    "    for archivo in os.listdir('Data'):\n",
    "        # Comprobar si el archivo tiene la extensi√≥n deseada\n",
    "        if archivo.endswith('.stat'):\n",
    "            try:\n",
    "                # Borrar el archivo\n",
    "                os.remove(os.path.join('Data', archivo))\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo borrar el archivo {archivo}: {e}\")\n",
    "        elif archivo.endswith('.out'):\n",
    "            try:\n",
    "                # Borrar el archivo\n",
    "                os.remove(os.path.join('Data', archivo))\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo borrar el archivo {archivo}: {e}\")\n",
    "        elif archivo.endswith('.h3d'):\n",
    "            try:\n",
    "                # Borrar el archivo\n",
    "                os.remove(os.path.join('Data', archivo))\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo borrar el archivo {archivo}: {e}\")\n",
    "        elif archivo.endswith('.mvw'):\n",
    "            try:\n",
    "                # Borrar el archivo\n",
    "                os.remove(os.path.join('Data', archivo))               \n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo borrar el archivo {archivo}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDENAR LOS RESULTADOS POR NODOS Y ELEMENTOS\n",
    "from Utilities.data import procesar_datos_nodos\n",
    "from Utilities.data import procesar_datos_elementos\n",
    "for i in range(1,num_casos+1):\n",
    "    procesar_datos_nodos(i)\n",
    "for i in range(1,num_casos+1):\n",
    "    procesar_datos_elementos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAR LOS ARCHIVOS PT\n",
    "for i in range(1, num_casos+1):\n",
    "    name_nod = f'Data_csv\\\\Data_{i}_nodos.csv'\n",
    "    name_ele = f'Data_csv\\\\Data_{i}_elementos.csv'\n",
    "    \n",
    "    nod_name = pd.read_csv(name_nod)  \n",
    "    ele_name = pd.read_csv(name_ele)   \n",
    "    \n",
    "    neighbors_nod = nod_name[['Vecino(Abajo)', 'Vecino(Arriba)', 'Vecino(Derecha)', 'Vecino(Izquierda)']]\n",
    "    features_nod = nod_name[['CC', 'Fuerzas(x)', 'Fuerzas(y)']] \n",
    "    output_nod_df = nod_name[['Desplazamientos(x)', 'Desplazamientos(y)', 'Desplazamientos(z)', 'Desplazamientos(rx)', 'Desplazamientos(ry)', 'Desplazamientos(rz)']]\n",
    "    #features_ele = ele_name[['Nodo_1', 'Nodo_2', 'Nodo_3', 'Nodo_4', 'Desplazamiento(x1)', 'Desplazamiento(y1)', 'Desplazamiento(x2)', 'Desplazamiento(y2)', 'Desplazamiento(x3)', 'Desplazamiento(y3)', 'Desplazamiento(x4)', 'Desplazamiento(y4)']]\n",
    "    #output_ele_df = ele_name[['Tensiones(VON)', 'Tensiones(XX1)', 'Tensiones(XX2)', 'Tensiones(YY1)', 'Tensiones(YY2)', 'Tensiones(XY1)', 'Tensiones(XY2)', 'Deformaciones(VON)', 'Deformacones(XX1)', 'Deformaciones(XX2)', 'Deformaciones(YY1)', 'Deformaciones(YY2)', 'Deformaciones(XY1)', 'Deformaciones(XY2)']]\n",
    "\n",
    "    neighbors_nod = torch.tensor(neighbors_nod.values)\n",
    "    input_nod = torch.tensor(features_nod.values)\n",
    "    output_nod = torch.tensor(output_nod_df.values)\n",
    "    \n",
    "    #input_ele = torch.tensor(features_ele.values) \n",
    "    #output_ele = torch.tensor(output_ele_df.values)\n",
    "\n",
    "    nod_dict = { f'tensor_nodos_ne_({i},)': neighbors_nod, f'tensor_nodos_in_({i},)': input_nod, f'tensor_nodos_out_({i},)': output_nod} #f'tensor_elementos_in_({i},)': input_ele}\n",
    "    #ele_dict = {f'tensor_elementos_in_({i},)': input_ele, f'tensor_elementos_out_({i},)': output_ele}\n",
    "    \n",
    "    torch.save((nod_dict), os.path.join('Data_pt', f'Data_({i},).pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
